{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b98ff01d",
   "metadata": {},
   "source": [
    "Siguiendo https://www.youtube.com/watch?v=kCc8FmEb1nY&t=1535s. \n",
    "\n",
    "Status: 9:25, tokenization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6bf8172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-01-14 10:20:23--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
      "\n",
      "2026-01-14 10:20:23 (16.9 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Comenzamos descargando el dataset de tiny Shakespeare para una primera implementación.\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ec497e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el archivo\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c90dcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n"
     ]
    }
   ],
   "source": [
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb1d5455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Así se ve el dataset\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf200ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# En forma cruda\n",
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62e6aed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n"
     ]
    }
   ],
   "source": [
    "# Cuántos caracteres?\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "952771c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# Definimos el vocabulario\n",
    "# Ordernados de acuerdo al código Unicode\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(chars)\n",
    "print(\"\".join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33c4a989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora nos preocupamos de la tokenización\n",
    "# Transformar los caracteres de texto, strings, a una serie de números enteros\n",
    "\n",
    "# Por simplicidad, usaremos un character-level tokenizer.\n",
    "# Nota: el tamaño del diccionario y el tamaño de la secuecia de tokens son inversamente proporcionales:\n",
    "# Puedes tener un diccionarioo vocabulario es pequeño (e.g. ~65 tokens para representar letras, números, y símbolos) y una secuencia de tokens más larga.\n",
    "# Como ejemplo, el tokenizador te GPT2 tiene ~50k tokens. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f71ebd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creemos un mapeo desde los caracteres a los enteros\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4b60fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 53, 50, 39]\n",
      "'z!.\n",
      "hola\n"
     ]
    }
   ],
   "source": [
    "# Encoder y decode de strings\n",
    "\n",
    "def encode(s):\n",
    "    return [stoi[ch] for ch in s]\n",
    "\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "# Ejemplos:\n",
    "\n",
    "print(encode(\"hola\"))\n",
    "print(decode([5, 64, 2, 8]))\n",
    "print(decode(encode(\"hola\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2c7fc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# Aplicamos el encoder a todo el texto\n",
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long) # .long sets the int precision to 64 bits, standard in PyTorch\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d1ee3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos ahora el tr/val split\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "737ad840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Ahora, nos disponemos a entrenar el modelo. Nunca entrenamos en todo el texto porque sería\n",
    "computacionalmente inviable.\n",
    "\n",
    "Lo que hacemos en realidad es entrenar en pequeños pedazos de texto del training dataset. Estos pedazos tienen un largo máximo,\n",
    "que se llama block_size.\n",
    "\"\"\"\n",
    "\n",
    "block_size = 8\n",
    "print(data[:block_size+1])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb63c25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47])\n",
      "tensor([47, 56, 57, 58,  1, 15, 47, 58])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "En una secuencia de 9 tokens, hay muchos ejemplos de cómo se organizan los tokens.\n",
    "\n",
    "En una secuencia de 18, 47 probablemente sigue. En una secuencia de 18, 47, 56 probablemente sigue. Y así. Hay varios contextos.\n",
    "\n",
    "En general, en una secuencia de N tokens, hay N-1 ejemplos que el transformer puede aprender.\n",
    "\"\"\"\n",
    "\n",
    "# Definimos el contexto\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63f3d4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuando el contexto es [18] el target es 47\n",
      "Cuando el contexto es [18, 47] el target es 56\n",
      "Cuando el contexto es [18, 47, 56] el target es 57\n",
      "Cuando el contexto es [18, 47, 56, 57] el target es 58\n",
      "Cuando el contexto es [18, 47, 56, 57, 58] el target es 1\n",
      "Cuando el contexto es [18, 47, 56, 57, 58, 1] el target es 15\n",
      "Cuando el contexto es [18, 47, 56, 57, 58, 1, 15] el target es 47\n",
      "Cuando el contexto es [18, 47, 56, 57, 58, 1, 15, 47] el target es 58\n"
     ]
    }
   ],
   "source": [
    "# Iteramos sobre tokens\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"Cuando el contexto es {context.tolist()} el target es {target.item()}\")\n",
    "\n",
    "# Es útil entrenar sobre contexts de distinta longitud para que el transformer aprenda\n",
    "# a hacer predicciones incluso con el contexto más corto, context = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b8fa09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora, lo podemos generalizar\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # cuantas secuencias procesamos en paralelo encada forward/backward pass\n",
    "block_size = 8 # context size\n",
    "\n",
    "def get_batch(split):\n",
    "    \"\"\"\n",
    "    Generate un pedazo de data con inputs x y output y.\n",
    "    \"\"\"\n",
    "\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    # Genera las posiciones random para extraer el pedazo de texto de ahí.\n",
    "    # El limite superior de los indices puede ser len(data) - block_size (si no, se saldría del rango)\n",
    "    # El limite inferior es 0 for default.\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b05c2f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "outputs\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "cuando el contexto es [24] el target es 43\n",
      "cuando el contexto es [24, 43] el target es 58\n",
      "cuando el contexto es [24, 43, 58] el target es 5\n",
      "cuando el contexto es [24, 43, 58, 5] el target es 57\n",
      "cuando el contexto es [24, 43, 58, 5, 57] el target es 1\n",
      "cuando el contexto es [24, 43, 58, 5, 57, 1] el target es 46\n",
      "cuando el contexto es [24, 43, 58, 5, 57, 1, 46] el target es 43\n",
      "cuando el contexto es [24, 43, 58, 5, 57, 1, 46, 43] el target es 39\n",
      "cuando el contexto es [44] el target es 53\n",
      "cuando el contexto es [44, 53] el target es 56\n",
      "cuando el contexto es [44, 53, 56] el target es 1\n",
      "cuando el contexto es [44, 53, 56, 1] el target es 58\n",
      "cuando el contexto es [44, 53, 56, 1, 58] el target es 46\n",
      "cuando el contexto es [44, 53, 56, 1, 58, 46] el target es 39\n",
      "cuando el contexto es [44, 53, 56, 1, 58, 46, 39] el target es 58\n",
      "cuando el contexto es [44, 53, 56, 1, 58, 46, 39, 58] el target es 1\n",
      "cuando el contexto es [52] el target es 58\n",
      "cuando el contexto es [52, 58] el target es 1\n",
      "cuando el contexto es [52, 58, 1] el target es 58\n",
      "cuando el contexto es [52, 58, 1, 58] el target es 46\n",
      "cuando el contexto es [52, 58, 1, 58, 46] el target es 39\n",
      "cuando el contexto es [52, 58, 1, 58, 46, 39] el target es 58\n",
      "cuando el contexto es [52, 58, 1, 58, 46, 39, 58] el target es 1\n",
      "cuando el contexto es [52, 58, 1, 58, 46, 39, 58, 1] el target es 46\n",
      "cuando el contexto es [25] el target es 17\n",
      "cuando el contexto es [25, 17] el target es 27\n",
      "cuando el contexto es [25, 17, 27] el target es 10\n",
      "cuando el contexto es [25, 17, 27, 10] el target es 0\n",
      "cuando el contexto es [25, 17, 27, 10, 0] el target es 21\n",
      "cuando el contexto es [25, 17, 27, 10, 0, 21] el target es 1\n",
      "cuando el contexto es [25, 17, 27, 10, 0, 21, 1] el target es 54\n",
      "cuando el contexto es [25, 17, 27, 10, 0, 21, 1, 54] el target es 39\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch(\"train\")\n",
    "\n",
    "print(\"inputs\")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"outputs\")\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print(\"----\")\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"cuando el contexto es {context.tolist()} el target es {target.item()}\")\n",
    "\n",
    "# En este caso, tenemos un set de 32 tokens (independientes, hasta donde el transformer sabe)\n",
    "# que alimentan al modelo en paralelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da81e87c",
   "metadata": {},
   "source": [
    "Implementaremos ahora uno de los modelos de lenguaje más básicos que hay: el bigram language model, que estima la probabilidad del próximo token en una secuencia considerando solamente el que le precede inmediatamente. Cada palabra depende sólo en la palabra anterior, por lo tanto es \"bi-gram\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d0345fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        En general, se tiene nn.Embedding(V, D), donde V es el número de tokens en el vocabulario y D es la dimensión del embedding space.\n",
    "        \"\"\"\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    # Recuerda que el nombre forward es especial en PyTorch. Si haces model(x) es como hacer model.forward(x), pero muy optimizado.\n",
    "    def forward(self, idx, targets=None):\n",
    "        # Tanto idx como targets son tensores de dimensions (B, T) de numeros enteros. \n",
    "        # Recuerda que los logits son los scores para cada token en el vocabulario.\n",
    "        logits = self.token_embedding_table(idx) # (B, T, C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "\n",
    "        else: \n",
    "            # La documentacion de la functional cross entropy en pytorch require que el array\n",
    "            # se entregue en otras dimensiones. Es un poco raro pero así es. Por lo tanto, usamos\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    # Ahora, debemos agregar un método que sea capaz de generar tokens a partir de un contexto dado.\n",
    "    # En la práctica, generaremos idx de dimensiones B por T, T+1, T+2, ...\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx es el (B, T) array de indices en el contexto actual\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Hacer las predicciones\n",
    "            logits, loss = self.forward(idx)\n",
    "            # Sólo en el último step de tiempo\n",
    "            logits = logits[:, -1, :] # se transforme en (B, C)\n",
    "            # Aplicamos softmax para obtener las probabilidades\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # Sampleamos ahora de la distribución, sólo el siguiente token\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # Concatenamos el nuevo token al contexto\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d131986d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
       "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
       "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
       "        [25, 17, 27, 10,  0, 21,  1, 54]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08267bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
       "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
       "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
       "        [17, 27, 10,  0, 21,  1, 54, 39]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "724f4edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor([[ 1.6347, -0.0518,  0.4996,  ...,  0.2432,  1.1519,  0.9950],\n",
      "        [ 0.3418, -0.9276,  1.2381,  ...,  1.5018, -0.5266,  0.2354],\n",
      "        [ 0.1479, -0.4333,  0.5203,  ...,  0.3302,  1.5454,  1.3778],\n",
      "        ...,\n",
      "        [-0.5693, -0.0735,  0.7743,  ..., -0.0815, -1.1445, -0.0623],\n",
      "        [ 0.4658, -0.2573, -1.0673,  ...,  1.2439,  1.3471,  1.6910],\n",
      "        [-0.4553,  0.0139,  0.9309,  ...,  0.0290, -0.7568,  0.8701]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor(5.0364, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "\n",
    "print(logits.shape)\n",
    "print(logits)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b9fa8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "l-QYjt'CL?jLDuQcLzy'RIo;'KdhpV\n",
      "vLixa,nswYZwLEPS'ptIZqOZJ$CA$zy-QTkeMk x.gQSFCLg!iW3fO!3DGXAqTsq3pdgq\n"
     ]
    }
   ],
   "source": [
    "# Creamos un pequeño tensor que tiene un zero. De ahí partimos la generación porque\n",
    "# el token que corresponde al token 0 es el newline character \\.\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c296be5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora comienza el entrenamiento\n",
    "# Inicializamos un optimizer object\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1c57b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5589, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for step in range(10000):\n",
    "    # Sampleamos un batch de datos\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    # Evaluamos la loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81569b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ong h hasbe pave pirance\n",
      "RDe hicomyonthar's\n",
      "PES:\n",
      "AKEd ith henourzincenonthioneir thondy, y heltieiengerofo'dsssit ey\n",
      "KINld pe wither vouprroutherccnohathe; d!\n",
      "My hind tt hinig t ouchos tes; st yo hind wotte grotonear 'so itJas\n",
      "Waketancotha:\n",
      "h hay.JUCLUKn prids, r loncave w hollular s O:\n",
      "HIs; ht anjx\n"
     ]
    }
   ],
   "source": [
    "# Ya tenemos una loss un poco mejor. Veamos qué tokens genera el bigram model por ahora\n",
    "print(decode(m.generate(idx, max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6b15ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nota que ya ha mejorado la performance. Entiende que después de cada \\n newline se comienza con una mayúscula,\n",
    "# El largo de las palabras ya esta mas regularizado también."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4de721",
   "metadata": {},
   "source": [
    "Un truco matemático para self attention. \n",
    "\n",
    "Para darle contexto a la next token prediction, crearemos una bag of words y usaremos su promedio para inferencia. El promedio, sin embargo, puede ser lento de calcula si no se vectoriza. Veamos cómo hacerlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90da0aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de juguete\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4, 8, 2 # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1d5f7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Queremos x[b, t] = promedio(i<t) * x[b, i]\n",
    "# Bag of words\n",
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # t, C\n",
    "        xbow[b, t] = torch.mean(xprev, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb64a084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[[ 0.1808, -0.0700],\n",
      "         [-0.0894, -0.4926],\n",
      "         [ 0.1490, -0.3199],\n",
      "         [ 0.3504, -0.2238],\n",
      "         [ 0.3525,  0.0545],\n",
      "         [ 0.0688, -0.0396],\n",
      "         [ 0.0927, -0.0682],\n",
      "         [-0.0341,  0.1332]],\n",
      "\n",
      "        [[ 1.3488, -0.1396],\n",
      "         [ 0.8173,  0.4127],\n",
      "         [-0.1342,  0.4395],\n",
      "         [ 0.2711,  0.4774],\n",
      "         [ 0.2421,  0.0694],\n",
      "         [ 0.0084,  0.0020],\n",
      "         [ 0.0712, -0.1128],\n",
      "         [ 0.2527,  0.2149]],\n",
      "\n",
      "        [[-0.6631, -0.2513],\n",
      "         [ 0.1735, -0.0649],\n",
      "         [ 0.1685,  0.3348],\n",
      "         [-0.1621,  0.1765],\n",
      "         [-0.2312, -0.0436],\n",
      "         [-0.1015, -0.2855],\n",
      "         [-0.2593, -0.1630],\n",
      "         [-0.3015, -0.2293]],\n",
      "\n",
      "        [[ 1.6455, -0.8030],\n",
      "         [ 1.4985, -0.5395],\n",
      "         [ 0.4954,  0.3420],\n",
      "         [ 1.0623, -0.1802],\n",
      "         [ 1.1401, -0.4462],\n",
      "         [ 1.0870, -0.4071],\n",
      "         [ 1.0430, -0.1299],\n",
      "         [ 1.1138, -0.1641]]])\n"
     ]
    }
   ],
   "source": [
    "# Usaremos las matrix multiplication de Pytorch, muy optimizadas.\n",
    "# Podemos recuperar el pasado de un token usando traingular matrices. Defininos weights\n",
    "wei = torch.tril(torch.ones((T, T)))\n",
    "print(wei)\n",
    "\n",
    "# Normalizamos\n",
    "wei = wei / wei.sum(dim=1, keepdim=True)\n",
    "xbow2 = wei @ x\n",
    "print(xbow2)\n",
    "# Arriba, la multiplciatión arriba es (T, T) @ (B, T, C).\n",
    "# Por el broadcasting, Pytorch agregará un índice al principio para que el cálculo se repita por batches.\n",
    "# Queda entonces (B, T, T) @ (B, T, C) ----> (B, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1647f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generaremos ahora otra forma de implementar la self attention usando Softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "# Poner el future a minus infinity indica que el current token sencillamente del contexto pasado\n",
    "# Despues de usat softmax en -inf sencillmanet obtendrás 0 affinity con el futuro.\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow2, xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e23158f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simpletoptagger",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
